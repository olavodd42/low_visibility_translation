{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bdea012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eeb7a294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdftotext\n",
    "\n",
    "def convert_to_txt(pdf_file, txt_file):\n",
    "    # Load your PDF\n",
    "    with open(pdf_file, \"rb\") as f:\n",
    "        pdf = pdftotext.PDF(f)\n",
    "\n",
    "#     # Read some individual pages\n",
    "#     print(pdf[0])\n",
    "#     print(pdf[1])\n",
    "    with open(txt_file, \"w\") as f:\n",
    "        for page in pdf:\n",
    "            f.write(page)\n",
    "\n",
    "convert_to_txt(\"data/eng-t4t_all.pdf\", \"data/eng.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9193baa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_txt(\"data/txuNT_all.pdf\", \"data/txu.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d52e915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize(txt1, txt2, res_file):\n",
    "    with open(txt1, encoding=\"utf-8\") as f_en, open(txt2, encoding=\"utf-8\") as f_txu:\n",
    "        eng_lines = f_en.readlines()\n",
    "        txu_lines = f_txu.readlines()\n",
    "\n",
    "    with open(res_file, \"w\", encoding=\"utf-8\") as out:\n",
    "        for en, tx in zip(eng_lines, txu_lines):\n",
    "            out.write(f\"{en.strip()}\\t{tx.strip()}\\n\")\n",
    "        \n",
    "parallelize(\"data/eng.txt\", \"data/txu.txt\", \"data/parallel.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef7ff1ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'copyright Â© 2012 Wycliffe Bible Translators, Inc.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_data = []\n",
    "txu_data = []\n",
    "with open(\"data/parallel.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line.strip('\\n')\n",
    "        eng, txu = line.split('\\t')\n",
    "        eng_data.append(eng)\n",
    "        txu_data.append(txu)\n",
    "        \n",
    "eng_data[5]\n",
    "txu_data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "065a916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files=\"data/parallel.txt\",\n",
    "    delimiter=\"\\t\",\n",
    "    column_names=[\"en\",\"txu\"],\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "data = data.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc49b424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['en', 'txu'],\n",
      "        num_rows: 54225\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['en', 'txu'],\n",
      "        num_rows: 6026\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ae6d8eb-c63d-41c8-afb6-869291a55d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50a4effdf8f4a6aa9268ab265692edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/54225 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c8e333720a4194ac2ad075f9da112f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6026 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "train_dataset = data['train']\n",
    "test_dataset = data['test']\n",
    "aug = naw.SynonymAug(aug_src='wordnet')\n",
    "\n",
    "def augment_batch(batch):\n",
    "    return {\n",
    "        'en': [aug.augment(text) for text in batch['en']],\n",
    "        'txu': batch['txu']\n",
    "    }\n",
    "\n",
    "train_aug_dataset = train_dataset.map(augment_batch, batched=True, batch_size=32)\n",
    "test_aug_dataset = test_dataset.map(augment_batch, batched=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acb972e2-d287-48c6-9912-7798cf8e0ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f0733aaa3a4898bf66ba0a735e3bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/54225 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bfef09161cd43648490ee97b4027b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6026 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "def join_en(example):\n",
    "    if isinstance(example[\"en\"], list):\n",
    "        return {\"en\": \" \".join(example[\"en\"])}\n",
    "    return example\n",
    "\n",
    "train_aug_dataset = train_aug_dataset.map(join_en)\n",
    "test_aug_dataset = test_aug_dataset.map(join_en)\n",
    "\n",
    "train_dataset = concatenate_datasets([train_dataset, train_aug_dataset])\n",
    "test_dataset = concatenate_datasets([test_dataset, test_aug_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc6c27d7-06dd-4f58-85f9-1e9e596b14cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885ca73befb341599e4df9616800d868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/108450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "895855c5d86c43529d83907553707309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12052 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "aug = naw.RandomWordAug(action=\"swap\")\n",
    "\n",
    "def augment_batch(batch):\n",
    "    return {\n",
    "        'en': [aug.augment(text) for text in batch['en']],\n",
    "        'txu': batch['txu']\n",
    "    }\n",
    "\n",
    "train_aug_dataset = train_dataset.map(augment_batch, batched=True, batch_size=32)\n",
    "test_aug_dataset = test_dataset.map(augment_batch, batched=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81525af0-9fec-445e-908c-430496eff5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23ea12513c944831aed83b304db5d34d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/108450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f1a180e25a42a5986f95021495810f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12052 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_aug_dataset = train_aug_dataset.map(join_en)\n",
    "test_aug_dataset = test_aug_dataset.map(join_en)\n",
    "\n",
    "train_dataset = concatenate_datasets([train_dataset, train_aug_dataset])\n",
    "test_dataset = concatenate_datasets([test_dataset, test_aug_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc46a359-f608-4520-896a-9ac6f3c71cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['en', 'txu'],\n",
       "    num_rows: 216900\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77c5e71e-7766-4ffd-ae9c-ec0bdc75bb5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d7311ab85e40559113ce15e89b0367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/216900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f35061fa59641509b0360f98e8ca0df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/6026 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset.shuffle(seed=42),\n",
    "    'test': data['test']\n",
    "})\n",
    "\n",
    "dataset.save_to_disk(\"augmented_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6961ac23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633a6889ea79492c8d39ffe30ca495cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/54225 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4177f1a787c8466d9315b25a1a4eba2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/6026 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142f24c87b93495c9bc87d6864a9c5bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/41519 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a811e0fc0ec496ea474c92f2e51edae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4609 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "\n",
    "def preprocess(example):\n",
    "    # prefix determine translation direction\n",
    "    prefix = \"<en>\"\n",
    "    enc = tokenizer(prefix + \" \" + example[\"en\"],\n",
    "                    truncation=True, max_length=128)\n",
    "    dec = tokenizer(example[\"txu\"],\n",
    "                    truncation=True, max_length=128)\n",
    "    return {\n",
    "        \"input_ids\": enc.input_ids,\n",
    "        \"attention_mask\": enc.attention_mask,\n",
    "        \"labels\": dec.input_ids\n",
    "    }\n",
    "\n",
    "# filter null lines\n",
    "data = data.filter(lambda x: x[\"en\"] is not None and x[\"txu\"] is not None)\n",
    "tokenized = data.map(preprocess, remove_columns=[\"en\",\"txu\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb6f4f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 41519\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 4609\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4384fed2-89e5-4e31-9c82-40bc76f00c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f487b423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a055cb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "def preprocess_text_parallel(texts, n_jobs = None, method = 'process'):\n",
    "    if n_jobs is None:\n",
    "        n_jobs = cpu_count()\n",
    "    \n",
    "    if method == 'process':\n",
    "        with ProcessPoolExecutor(max_workers=n_jobs) as executor:\n",
    "            results = list(executor.map(preprocess_text, texts))\n",
    "    elif method == 'thread':\n",
    "        with ThreadPoolExecutor(max_workers=n_jobs) as executor:\n",
    "            results = list(executor.map(preprocess_text, texts))\n",
    "    else:\n",
    "        raise ValueError(\"method deve ser 'process' ou 'thread'\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3d0c757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ii'], [], ['translation', 'for', 'translators'], [], ['a', 'bible', 'translation', 'for', 'bible', 'translators', 'which', 'makes', 'implied', 'information', 'explicit', 'in', 'the'], ['text', 'as', 'an', 'aid', 'to', 'the', 'translator', 'who', 'may', 'need', 'that', 'information', 'to', 'correctly', 'translate', 'into', 'a'], ['particular', 'language'], [], ['copyright', 'ellis', 'w', 'deibler', 'jr'], ['language', 'english']]\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "\n",
    "def preprocess_text_vectorized(texts):\n",
    "    \"\"\"\n",
    "    Otimize using vectorization\n",
    "    \"\"\"\n",
    "    # Batch transforms\n",
    "    texts_lower = [text.lower() for text in texts]\n",
    "    \n",
    "    # Remove digits and specials chars in batch\n",
    "    cleaned_texts = []\n",
    "    for text in texts_lower:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        cleaned_texts.append(text)\n",
    "    \n",
    "    # parallel tokenize\n",
    "    return preprocess_text_parallel(cleaned_texts, method='thread')\n",
    "\n",
    "eng_tokens = preprocess_text_vectorized(eng_data)\n",
    "print(eng_tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f8e56af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60251"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eng_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a23aedd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60251"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txu_tokens = preprocess_text_vectorized(txu_data)\n",
    "len(txu_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1c8111d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nhym', 'kam', 'djekonij', 'ará»³m', 'xarati']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txu_tokens[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2060be38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['that',\n",
       " 'are',\n",
       " 'different',\n",
       " 'from',\n",
       " 'every',\n",
       " 'other',\n",
       " 'languagejust',\n",
       " 'like',\n",
       " 'every',\n",
       " 'language',\n",
       " 'has',\n",
       " 'a',\n",
       " 'set',\n",
       " 'of']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_tokens[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ec64cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (8.21.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.9.0)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m216.6/216.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.7 jupyterlab_widgets-3.0.15 widgetsnbextension-4.0.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install ipywidgets\n",
    "jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "810b4abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing /usr/local/lib/python3.10/dist-packages/widgetsnbextension/static -> jupyter-js-widgets\n",
      "Up to date: /usr/local/share/jupyter/nbextensions/jupyter-js-widgets/extension.js.map\n",
      "Up to date: /usr/local/share/jupyter/nbextensions/jupyter-js-widgets/extension.js\n",
      "Up to date: /usr/local/share/jupyter/nbextensions/jupyter-js-widgets/extension.js.LICENSE.txt\n",
      "- Validating: \u001b[32mOK\u001b[0m\n",
      "\n",
      "    To initialize this nbextension in the browser every time the notebook (or other app) loads:\n",
      "    \n",
      "          jupyter nbextension enable widgetsnbextension --py\n",
      "    \n",
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "jupyter nbextension install --py widgetsnbextension\n",
    "jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d44488d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[256047, 13374, 1398, 4260, 4039, 248130, 2]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from transformers import NllbTokenizer\n",
    "\n",
    "tokenizer = NllbTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "tokenizer(\"How was your day?\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b750d42c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
