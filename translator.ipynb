{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeb7a294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['kayapo', 'ingles'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carrega o CSV\n",
    "df = pd.read_csv('data/txu_samples_processado.csv')\n",
    "\n",
    "# Remove a coluna pelo nome\n",
    "df = df.drop(columns=['id', 'linha_original'])\n",
    "df.to_csv('data/txu_samples.csv', index=False)\n",
    "\n",
    "# Salva o CSV sem a coluna\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c33dc01-e984-4511-8474-c92f4639d1bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1465740890.py, line 6)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(f\"PRED: {row.prediction}\\n\"\u001b[39m\n                                     ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"evaluation_results.csv\")\n",
    "for idx, row in df.sample(10, random_state=42).iterrows():\n",
    "    print(f\"REF: {row.reference}\")\n",
    "    print(f\"PRED: {row.prediction}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9193baa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_txt(\"data/txuNT_all.pdf\", \"data/txu.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d52e915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize(txt1, txt2, res_file):\n",
    "    with open(txt1, encoding=\"utf-8\") as f_en, open(txt2, encoding=\"utf-8\") as f_txu:\n",
    "        eng_lines = f_en.readlines()\n",
    "        txu_lines = f_txu.readlines()\n",
    "\n",
    "    with open(res_file, \"w\", encoding=\"utf-8\") as out:\n",
    "        for en, tx in zip(eng_lines, txu_lines):\n",
    "            out.write(f\"{en.strip()}\\t{tx.strip()}\\n\")\n",
    "        \n",
    "parallelize(\"data/eng.txt\", \"data/txu.txt\", \"data/parallel.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef7ff1ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'copyright © 2012 Wycliffe Bible Translators, Inc.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_data = []\n",
    "txu_data = []\n",
    "with open(\"data/parallel.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line.strip('\\n')\n",
    "        eng, txu = line.split('\\t')\n",
    "        eng_data.append(eng)\n",
    "        txu_data.append(txu)\n",
    "        \n",
    "eng_data[5]\n",
    "txu_data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "065a916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files=\"data/parallel.txt\",\n",
    "    delimiter=\"\\t\",\n",
    "    column_names=[\"en\",\"txu\"],\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "data = data.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc49b424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['en', 'txu'],\n",
      "        num_rows: 54225\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['en', 'txu'],\n",
      "        num_rows: 6026\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ae6d8eb-c63d-41c8-afb6-869291a55d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50a4effdf8f4a6aa9268ab265692edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/54225 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c8e333720a4194ac2ad075f9da112f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6026 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "train_dataset = data['train']\n",
    "test_dataset = data['test']\n",
    "aug = naw.SynonymAug(aug_src='wordnet')\n",
    "\n",
    "def augment_batch(batch):\n",
    "    return {\n",
    "        'en': [aug.augment(text) for text in batch['en']],\n",
    "        'txu': batch['txu']\n",
    "    }\n",
    "\n",
    "train_aug_dataset = train_dataset.map(augment_batch, batched=True, batch_size=32)\n",
    "test_aug_dataset = test_dataset.map(augment_batch, batched=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acb972e2-d287-48c6-9912-7798cf8e0ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f0733aaa3a4898bf66ba0a735e3bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/54225 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bfef09161cd43648490ee97b4027b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6026 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "def join_en(example):\n",
    "    if isinstance(example[\"en\"], list):\n",
    "        return {\"en\": \" \".join(example[\"en\"])}\n",
    "    return example\n",
    "\n",
    "train_aug_dataset = train_aug_dataset.map(join_en)\n",
    "test_aug_dataset = test_aug_dataset.map(join_en)\n",
    "\n",
    "train_dataset = concatenate_datasets([train_dataset, train_aug_dataset])\n",
    "test_dataset = concatenate_datasets([test_dataset, test_aug_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc6c27d7-06dd-4f58-85f9-1e9e596b14cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885ca73befb341599e4df9616800d868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/108450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "895855c5d86c43529d83907553707309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12052 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "aug = naw.RandomWordAug(action=\"swap\")\n",
    "\n",
    "def augment_batch(batch):\n",
    "    return {\n",
    "        'en': [aug.augment(text) for text in batch['en']],\n",
    "        'txu': batch['txu']\n",
    "    }\n",
    "\n",
    "train_aug_dataset = train_dataset.map(augment_batch, batched=True, batch_size=32)\n",
    "test_aug_dataset = test_dataset.map(augment_batch, batched=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81525af0-9fec-445e-908c-430496eff5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23ea12513c944831aed83b304db5d34d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/108450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f1a180e25a42a5986f95021495810f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12052 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_dataset = concatenate_datasets([train_dataset, train_aug_dataset])\n",
    "test_dataset = concatenate_datasets([test_dataset, test_aug_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc46a359-f608-4520-896a-9ac6f3c71cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['en', 'txu'],\n",
       "    num_rows: 216900\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77c5e71e-7766-4ffd-ae9c-ec0bdc75bb5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d7311ab85e40559113ce15e89b0367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/216900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f35061fa59641509b0360f98e8ca0df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/6026 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset.shuffle(seed=42),\n",
    "    'test': data['test']\n",
    "})\n",
    "\n",
    "dataset.save_to_disk(\"augmented_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f645d7b-1817-4616-8eae-aec906c57026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "# Resolver problema do tokenizers|\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Desabilitar alguns warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab450da8-6183-4974-8e78-e22f46f0d755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['en', 'txu'],\n",
      "        num_rows: 216900\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['en', 'txu'],\n",
      "        num_rows: 6026\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from transformers.integrations import deepspeed\n",
    "deepspeed._hf_deepspeed_config_weak_ref = None\n",
    "\n",
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk(\"augmented_dataset\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6961ac23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0fc07a3ea314ed4bfe229f5ae861a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/312M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8311dec5398490dbc06fb483d12cba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a4bc50a1604dd1a5ab7f245b832fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/312M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf25d438f6ea4d6f833f2d4a16dbd7f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/54225 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e46dd3276a8454c91b2303eda3e6afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/6026 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca28182790f495a821c3bbedf183191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/41544 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70fdf3b94cef491ea3b6e74ccb5bd060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4584 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_FP16 = torch.cuda.is_available()\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-ROMANCE\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(DEVICE)\n",
    "\n",
    "# Carrega dataset e divide em treino/teste\n",
    "raw_dataset = load_dataset(\"csv\", data_files=\"data/parallel.txt\", delimiter=\"\\t\", column_names=[\"en\", \"txu\"], split=\"train\")\n",
    "dataset = raw_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# Filtra entradas nulas em ambos splits\n",
    "dataset[\"train\"] = dataset[\"train\"].filter(lambda x: x[\"en\"] is not None and x[\"txu\"] is not None)\n",
    "dataset[\"test\"] = dataset[\"test\"].filter(lambda x: x[\"en\"] is not None and x[\"txu\"] is not None)\n",
    "\n",
    "# Pré-processamento\n",
    "def preprocess(example):\n",
    "    inputs = tokenizer(example[\"en\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    targets = tokenizer(example[\"txu\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "train_dataset = dataset[\"train\"].map(preprocess, batched=True)\n",
    "test_dataset = dataset[\"test\"].map(preprocess, batched=True)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb6f4f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['en', 'txu', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 41544\n",
      "})\n",
      "Dataset({\n",
      "    features: ['en', 'txu', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 4584\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8be825f5-7b3e-403f-8294-a04bd49eaed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "import gc\n",
    "import os\n",
    "import torch\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU memory\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def print_memory_usage():\n",
    "    \"\"\"Print current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**2\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**2\n",
    "        print(f\"GPU Memory - Allocated: {allocated:.2f} MB, Reserved: {reserved:.2f} MB\")\n",
    "\n",
    "clear_memory()\n",
    "\n",
    "# Initialize accelerator with memory optimizations\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients over 4 steps\n",
    "    mixed_precision=\"fp16\"  # Use half precision to save memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50a14936-4d26-47f9-b6aa-562a02e00b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = [[(l if l != -100 else tokenizer.pad_token_id) for l in label] for label in labels]\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {k: v.mid.fmeasure * 100 for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfa7a61-bd6c-43b2-807e-06769cb766f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"facebook/nllb-200-distilled-600M\",  # Use half precision\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.config.use_cache = False\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Add LoRA adapters instead of full fine-tuning\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Low rank\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=64  # Limit sequence length to save memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f6aafa5-a5ec-4a06-be47-d59cdd053c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA disponível: True\n",
      "GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "Memória GPU: 4.00 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA disponível: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memória GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fad17520-020e-4da8-86cf-a1bcada7e4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    tokenized_dataset[\"train\"],\n",
    "    batch_size=1,  # Keep batch size at 1\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    pin_memory=False,  # Disable pin_memory to save GPU memory\n",
    "    num_workers=0  # Disable multiprocessing to save memory\n",
    ")\n",
    "\n",
    "eval_loader = DataLoader(\n",
    "    tokenized_dataset[\"test\"],\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    pin_memory=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=3e-5,  # Slightly lower learning rate\n",
    "    weight_decay=0.01\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d13fd152-bb79-4a43-8cbd-eb06fbd84e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_loader) // accelerator.gradient_accumulation_steps\n",
    "\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_training_steps // 10,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "model, optimizer, train_loader, eval_loader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_loader, eval_loader, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5787335b-cd0b-46bf-b9af-0a08e501b22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 2358.84 MB, Reserved: 5368.00 MB\n",
      "Starting training...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m loss = outputs.loss / accelerator.gradient_accumulation_steps\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Optimizer step (only when gradients are accumulated)\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m accelerator.sync_gradients:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/accelerate/accelerator.py:2549\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2548\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2549\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2550\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_lomo_optimizer:\n\u001b[32m   2551\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "print_memory_usage()\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(train_loader):\n",
    "        with accelerator.accumulate(model):\n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss / accelerator.gradient_accumulation_steps\n",
    "            \n",
    "            # Backward pass\n",
    "            accelerator.backward(loss)\n",
    "            \n",
    "            # Optimizer step (only when gradients are accumulated)\n",
    "            if accelerator.sync_gradients:\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Clear memory periodically\n",
    "                if step % 50 == 0:\n",
    "                    clear_memory()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Print progress\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Step {step}, Loss: {loss.item():.4f}\")\n",
    "            print_memory_usage()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} | Train loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    # Validation with memory management\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(eval_loader):\n",
    "            outputs = model(**batch)\n",
    "            eval_loss += outputs.loss.item()\n",
    "            \n",
    "            # Clear memory during validation too\n",
    "            if step % 50 == 0:\n",
    "                clear_memory()\n",
    "    \n",
    "    avg_eval_loss = eval_loss / len(eval_loader)\n",
    "    print(f\"Epoch {epoch+1} | Eval loss: {avg_eval_loss:.4f}\")\n",
    "    print_memory_usage()\n",
    "    \n",
    "    # Clear memory after each epoch\n",
    "    clear_memory()\n",
    "\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92597b78-b3f8-46cb-9de2-4b945e95b67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_smaller_model():\n",
    "    \"\"\"Load a smaller NLLB model if the 600M version is too large\"\"\"\n",
    "    clear_memory()\n",
    "    \n",
    "    # Try the 200M version instead\n",
    "    model_name = \"facebook/nllb-200-200M\"  # Much smaller model\n",
    "    \n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Uncomment the following lines if you still get OOM errors:\n",
    "# print(\"Loading smaller model due to memory constraints...\")\n",
    "# model, tokenizer = load_smaller_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cef9cdf-c181-46a8-bba5-7ecf18d64784",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./modelo_final\")\n",
    "tokenizer.save_pretrained(\"./modelo_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37c8808-6b71-40f0-ba3f-852d7cb746da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
